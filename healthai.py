# -*- coding: utf-8 -*-
"""healthai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CQy0NrMXY4y2HOwpPxFpOoCuj09mXcMf
"""

!pip install transformers torch gradio -q

import os
import json
import tempfile
import threading
from typing import List, Tuple, Dict, Any, Optional

import torch
import pandas as pd
import plotly.express as px
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer

# -------------------------------
# Model Loader with graceful fallback
# -------------------------------

DEFAULT_MODEL = "ibm-granite/granite-3.2-2b-instruct"  # user can change in UI
FALLBACK_MODELS = [
    # Public examples you may have access to in your environment
    "HuggingFaceH4/zephyr-7b-beta",
    "mistralai/Mistral-7B-Instruct-v0.2",
]

MODEL_CACHE: Dict[str, Dict[str, Any]] = {}


def load_model(model_name: str):
    """Load (or reuse) tokenizer + model. Falls back if model fails to load."""
    if model_name in MODEL_CACHE:
        return MODEL_CACHE[model_name]["tok"], MODEL_CACHE[model_name]["mdl"]

    tried = []
    candidates = [model_name] + [m for m in FALLBACK_MODELS if m != model_name]

    for name in candidates:
        try:
            tok = AutoTokenizer.from_pretrained(name)
            mdl = AutoModelForCausalLM.from_pretrained(
                name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                low_cpu_mem_usage=True,
            )
            if tok.pad_token is None:
                tok.pad_token = tok.eos_token
            MODEL_CACHE[name] = {"tok": tok, "mdl": mdl}
            return tok, mdl
        except Exception as e:
            tried.append((name, str(e)))
            continue

    # If all fail, raise a concise error
    msg = "\n".join([f"- {n}: {err[:300]}" for n, err in tried])
    raise RuntimeError(f"Failed to load any model. Tried:\n{msg}")


# -------------------------------
# Guardrails
# -------------------------------

DANGEROUS_KEYWORDS = [
    "overdose",
    "unprescribed",
    "inject without prescription",
    "home abortion",
    "self-surgery",
]

DISCLAIMER = (
    "\n\n⚠ HealthAI provides general information only and is not a substitute for professional medical advice. "
    "Always consult a licensed clinician for diagnosis and treatment."
)

# -------------------------------
# Generation Utilities (Streaming)
# -------------------------------


def stream_generate(tokenizer, model, prompt: str, max_new_tokens: int = 512, temperature: float = 0.7):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
    if torch.cuda.is_available():
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)

    gen_kwargs = dict(
        input_ids=inputs["input_ids"],
        attention_mask=inputs.get("attention_mask", None),
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=temperature,
        pad_token_id=tokenizer.eos_token_id,
        streamer=streamer,
    )

    # Start generation on a background thread to allow streaming
    thread = threading.Thread(target=model.generate, kwargs=gen_kwargs)
    thread.start()
    # Yield tokens as they arrive from the streamer
    for token in streamer:
        yield token
    thread.join()


# -------------------------------
# Prompt Builders (structured outputs)
# -------------------------------


def system_prefix():
    return (
        "You are HealthAI, a careful, concise medical information assistant. "
        "Keep advice general and safe. Avoid making diagnoses. "
        "Use simple language and short paragraphs."
    )


def build_prediction_prompt(name, age, gender, history, medications, allergies, symptoms):
    return f"""
{system_prefix()}

Patient Details:
- Name: {name}
- Age: {age}
- Gender: {gender}
- Medical History: {history}
- Current Medications: {medications}
- Allergies: {allergies}
- Reported Symptoms: {symptoms}

Task: List possible common conditions and general self-care guidance.
Return ONLY valid JSON with keys: conditions (list of strings), red_flags (list of strings), recommendations (list of strings).
Do not include any other commentary outside JSON.
""".strip()


def build_treatment_prompt(name, age, gender, history, condition):
    return f"""
{system_prefix()}

Patient Details:
- Name: {name}
- Age: {age}
- Gender: {gender}
- Medical History: {history}

Condition: {condition}

Task: Provide a general treatment overview.
Return ONLY valid JSON with keys: overview (string), self_care (list of strings), when_to_seek_care (list of strings), lifestyle (list of strings).
Do not include any other commentary outside JSON.
""".strip()


def build_chat_prompt(name, age, gender, history, medications, allergies, question):
    return f"""
{system_prefix()}

Patient:
- Name: {name}
- Age: {age}
- Gender: {gender}
- Medical History: {history}
- Current Medications: {medications}
- Allergies: {allergies}

Question: {question}
Provide a short, friendly, safe answer in 3-6 bullet points.
""".strip()


# -------------------------------
# Post-processing helpers
# -------------------------------


def looks_dangerous(text: str) -> bool:
    low = text.lower()
    return any(k in low for k in DANGEROUS_KEYWORDS)


def parse_json_maybe(s: str) -> Dict[str, Any]:
    s = s.strip()
    # Try to find the first JSON object in the string
    start = s.find("{")
    end = s.rfind("}")
    if start != -1 and end != -1 and end > start:
        candidate = s[start: end + 1]
        try:
            return json.loads(candidate)
        except Exception:
            pass
    # Fallback
    return {}


def md_from_prediction(data: Dict[str, Any]) -> str:
    conditions = data.get("conditions", [])
    red_flags = data.get("red_flags", [])
    recs = data.get("recommendations", [])
    md = ["## Potential Conditions", *[f"- {c}" for c in conditions],
          "\n## Red Flags", *[f"- {r}" for r in red_flags],
          "\n## Recommendations", *[f"- {r}" for r in recs]]
    return "\n".join(md) + DISCLAIMER


def md_from_treatment(data: Dict[str, Any]) -> str:
    md = [
        "## Overview",
        data.get("overview", "Not available."),
        "\n## Self-care",
        *[f"- {x}" for x in data.get("self_care", [])],
        "\n## When to seek care",
        *[f"- {x}" for x in data.get("when_to_seek_care", [])],
        "\n## Lifestyle",
        *[f"- {x}" for x in data.get("lifestyle", [])],
    ]
    return "\n".join(md) + DISCLAIMER


def save_markdown(content: str, prefix: str = "healthai") -> str:
    fd, path = tempfile.mkstemp(suffix=".md", prefix=f"{prefix}_")
    with os.fdopen(fd, "w", encoding="utf-8") as f:
        f.write(content)
    return path


# -------------------------------
# Gradio Handlers
# -------------------------------


def handle_prediction(model_name, name, age, gender, history, medications, allergies, symptoms):
    if not str(symptoms).strip():
        yield None, "Please enter symptoms to analyze."
        return
    try:
        tok, mdl = load_model(model_name)
        prompt = build_prediction_prompt(name, age, gender, history, medications, allergies, symptoms)
        out_text = ""
        # stream partial JSON text to the Markdown output while generating
        for chunk in stream_generate(tok, mdl, prompt, max_new_tokens=512, temperature=0.7):
            out_text += chunk
            yield None, out_text  # live JSON stream (may be partial)
        # After generation finishes, try to parse JSON and return a markdown file path + formatted md
        data = parse_json_maybe(out_text)
        md = md_from_prediction(data) if data else ("Could not parse structured output.\n" + out_text + DISCLAIMER)
        file_path = save_markdown(md, prefix="prediction")
        yield file_path, md
    except Exception as e:
        yield None, f"Error: {e}"


def handle_treatment(model_name, name, age, gender, history, condition):
    if not str(condition).strip():
        yield None, "Please enter a condition (e.g., migraine)."
        return
    try:
        tok, mdl = load_model(model_name)
        prompt = build_treatment_prompt(name, age, gender, history, condition)
        out_text = ""
        for chunk in stream_generate(tok, mdl, prompt, max_new_tokens=500, temperature=0.7):
            out_text += chunk
            yield None, out_text
        data = parse_json_maybe(out_text)
        md = md_from_treatment(data) if data else ("Could not parse structured output.\n" + out_text + DISCLAIMER)
        file_path = save_markdown(md, prefix="treatment")
        yield file_path, md
    except Exception as e:
        yield None, f"Error: {e}"


def handle_chat(model_name, history_list: List[Tuple[str, str]], name, age, gender, history, medications, allergies, user_msg):
    if not str(user_msg).strip():
        return history_list

    try:
        tok, mdl = load_model(model_name)
        prompt = build_chat_prompt(name, age, gender, history, medications, allergies, user_msg)

        ai_accum = ""
        history_list = history_list + [(user_msg, "")]  # placeholder for streaming
        for chunk in stream_generate(tok, mdl, prompt, max_new_tokens=300, temperature=0.7):
            ai_accum += chunk
            # Guardrails live check
            display = "Unsafe content detected. Please consult a clinician." if looks_dangerous(ai_accum) else ai_accum
            history_list[-1] = (user_msg, display)
            yield history_list

        final_text = ai_accum + DISCLAIMER
        if looks_dangerous(ai_accum):
            final_text = "I can’t assist with that request. Please contact a healthcare professional immediately." + DISCLAIMER
        history_list[-1] = (user_msg, final_text)
        yield history_list
    except Exception as e:
        history_list = history_list + [(user_msg, f"Error: {e}")]
        return history_list


# -------------------------------
# Analytics
# -------------------------------


def default_vitals_df():
    return pd.DataFrame({
        "Day": ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"],
        "Heart Rate": [72, 76, 70, 74, 78, 80, 75],
        "Blood Pressure": [120, 125, 118, 122, 130, 128, 124],
        "Glucose": [90, 95, 92, 100, 98, 96, 94],
    })


def build_vitals_plot(df: pd.DataFrame):
    long_df = df.melt(id_vars=["Day"], var_name="Metric", value_name="Value")
    fig = px.line(long_df, x="Day", y="Value", color="Metric", markers=True, title="Weekly Health Vitals")
    return fig


def handle_analytics(file):
    try:
        if file is None:
            df = default_vitals_df()
        else:
            ext = os.path.splitext(file.name)[1].lower()
            if ext in [".csv", ".txt"]:
                df = pd.read_csv(file.name)
            elif ext in [".xlsx", ".xls"]:
                df = pd.read_excel(file.name)
            else:
                return None, "Unsupported file type. Please upload CSV or Excel."
        required = {"Day"}
        if not required.issubset(set(df.columns)):
            return None, "Your file must contain a 'Day' column."
        fig = build_vitals_plot(df)
        return fig, ""
    except Exception as e:
        return None, f"Error: {e}"


# -------------------------------
# UI
# -------------------------------

with gr.Blocks(theme=gr.themes.Soft(primary_hue=gr.themes.colors.blue)) as demo:
    gr.Markdown("## 🏥 HealthAI — Intelligent Healthcare Assistant")
    gr.Markdown("""
> ⚠ *Medical Disclaimer*: HealthAI is for educational purposes only and does not provide medical advice, diagnosis, or treatment. Always consult a licensed healthcare professional.
""")

    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### 👤 Patient Profile")
            name = gr.Textbox(label="Name", value="Rithvik")
            age = gr.Number(label="Age", value=22, precision=0)
            gender = gr.Dropdown(["Male", "Female", "Other"], label="Gender", value="Male")
            history = gr.Textbox(label="Medical History", value="None")
            medications = gr.Textbox(label="Current Medications", value="None")
            allergies = gr.Textbox(label="Allergies", value="None")

            gr.Markdown("### 🧩 Model Settings")
            model_name = gr.Textbox(label="Model Name", value=DEFAULT_MODEL)
            gr.Markdown("Tip: If the default model fails, the app will try a fallback.")

        with gr.Column(scale=3):
            with gr.Tab("🧪 Disease Prediction"):
                symptoms = gr.Textbox(label="Current Symptoms", placeholder="e.g., fever, headache, cough, fatigue...", lines=4)
                predict_btn = gr.Button("Generate Prediction", variant="primary")
                prediction_file = gr.File(label="Download Report (.md)")
                prediction_output = gr.Markdown(label="Potential Conditions (JSON will stream, then formatted report)")
                predict_btn.click(
                    handle_prediction,
                    inputs=[model_name, name, age, gender, history, medications, allergies, symptoms],
                    outputs=[prediction_file, prediction_output],
                )

            with gr.Tab("💊 Treatment Plan"):
                condition = gr.Textbox(label="Medical Condition", placeholder="e.g., Mouth Ulcer, Diabetes, Migraine...", lines=2)
                plan_btn = gr.Button("Generate Treatment Plan", variant="primary")
                plan_file = gr.File(label="Download Plan (.md)")
                plan_output = gr.Markdown(label="Personalized Treatment Plan")
                plan_btn.click(
                    handle_treatment,
                    inputs=[model_name, name, age, gender, history, condition],
                    outputs=[plan_file, plan_output],
                )

            with gr.Tab("💬 24/7 Patient Support"):
                chatbot = gr.Chatbot(height=320)
                chat_input = gr.Textbox(label="Ask your health question", placeholder="e.g., I have fever and headache. What should I do?")
                chat_btn = gr.Button("Ask", variant="primary")

                def clear_chat():
                    return []

                chat_clear = gr.Button("Clear Chat")
                chat_clear.click(fn=clear_chat, inputs=None, outputs=chatbot)

                chat_btn.click(
                    handle_chat,
                    inputs=[model_name, chatbot, name, age, gender, history, medications, allergies, chat_input],
                    outputs=chatbot,
                )

            with gr.Tab("📊 Analytics Dashboard"):
                gr.Markdown("### Health Vitals Overview")
                file_up = gr.File(label="Upload CSV/Excel (columns: Day, ...)")
                analytics_btn = gr.Button("Generate Health Report", variant="primary")
                analytics_plot = gr.Plot(label="Vitals Chart")
                analytics_msg = gr.Markdown(visible=True)

                analytics_btn.click(
                    handle_analytics,
                    inputs=[file_up],
                    outputs=[analytics_plot, analytics_msg],
                )

# For Gradio Spaces / local run
if __name__ == "__main__":
    demo.queue()
    demo.launch(server_name="0.0.0.0", server_port=7860, share=True)

